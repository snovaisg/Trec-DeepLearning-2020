{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ns5uapaPXMt1"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qtk2kaXUtmxS"
   },
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KKCENnQNj-Fr"
   },
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "j2sMxDFwXYwp",
    "outputId": "ce3ee4ec-3f03-4e12-ba77-da11cde4048d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-06 18:13:13--  https://msmarco.blob.core.windows.net/msmarcoranking/qrels.train.tsv\n",
      "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 40.112.152.16\n",
      "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|40.112.152.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10589532 (10M) [text/tab-separated-values]\n",
      "Saving to: ‘data/qrels.train.tsv’\n",
      "\n",
      "data/qrels.train.ts 100%[===================>]  10.10M  11.0MB/s    in 0.9s    \n",
      "\n",
      "2020-08-06 18:13:14 (11.0 MB/s) - ‘data/qrels.train.tsv’ saved [10589532/10589532]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget -O data/queries.tar.gz  https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz\n",
    "#!wget -O data/qrels.dev.tsv https://msmarco.blob.core.windows.net/msmarcoranking/qrels.dev.tsv\n",
    "#!wget -O data/qrels.train.tsv https://msmarco.blob.core.windows.net/msmarcoranking/qrels.train.tsv\n",
    "#!wget -O data/collection.tar.gz https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz\n",
    "#!wget -O data19/2019qrels-pass.txt https://trec.nist.gov/data/deep/2019qrels-pass.txt\n",
    "#!wget -O data19/queries.tar.gz https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSLvVgbpElam"
   },
   "source": [
    "# Unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_BZep7ofEsT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "def unzip_all_data():\n",
    "    cwd = os.getcwd()\n",
    "    path = os.path.join(cwd, 'data')   # Change according to your path \n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    # unzip queries\n",
    "\n",
    "    with tarfile.open('queries.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "    # unzip passages\n",
    "    with tarfile.open('collection.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "\n",
    "    path19 = os.path.join(cwd, 'data19')\n",
    "    os.chdir(path19)\n",
    "\n",
    "    with tarfile.open('queries.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "    os.chdir(cwd)\n",
    "    return None\n",
    "\n",
    "# uncomment if it is the first time unzipping the data\n",
    "# unzip_all_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVyLhAQhl05Q"
   },
   "source": [
    "# Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_JTaSHsl5Zp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing passage 0\n",
      "Processing passage 200000\n",
      "Processing passage 400000\n",
      "21:16:25.495 [main] WARN  o.t.structures.indexing.Indexer - Adding an empty document to the index (500080) - further warnings are suppressed\n",
      "Processing passage 600000\n",
      "Processing passage 800000\n",
      "Processing passage 1000000\n",
      "Processing passage 1200000\n",
      "Processing passage 1400000\n",
      "Processing passage 1600000\n",
      "Processing passage 1800000\n",
      "Processing passage 2000000\n",
      "Processing passage 2200000\n",
      "Processing passage 2400000\n",
      "Processing passage 2600000\n",
      "Processing passage 2800000\n",
      "Processing passage 3000000\n",
      "Processing passage 3200000\n",
      "Processing passage 3400000\n",
      "Processing passage 3600000\n",
      "Processing passage 3800000\n",
      "Processing passage 4000000\n",
      "Processing passage 4200000\n",
      "Processing passage 4400000\n",
      "Processing passage 4600000\n",
      "Processing passage 4800000\n",
      "Processing passage 5000000\n",
      "Processing passage 5200000\n",
      "Processing passage 5400000\n",
      "Processing passage 5600000\n",
      "Processing passage 5800000\n",
      "Processing passage 6000000\n",
      "Processing passage 6200000\n",
      "Processing passage 6400000\n",
      "Processing passage 6600000\n",
      "Processing passage 6800000\n",
      "Processing passage 7000000\n",
      "Processing passage 7200000\n",
      "Processing passage 7400000\n",
      "Processing passage 7600000\n",
      "Processing passage 7800000\n",
      "Processing passage 8000000\n",
      "Processing passage 8200000\n",
      "Processing passage 8400000\n",
      "Processing passage 8600000\n",
      "Processing passage 8800000\n",
      "22:02:50.921 [main] WARN  o.t.structures.indexing.Indexer - Indexed 5 empty documents\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def passages_generator(filepath : str, delimiter : str, verbose : bool=False):\n",
    "    \"\"\"\n",
    "    Generator of passages dataset. Generates 1 passage at a time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        path of file that contains passages in a csv format \n",
    "        with two fields: [passage_id] and [passage_text].\n",
    "\n",
    "    delimiter : str\n",
    "        delimiter of csv file that contains the passages\n",
    "\n",
    "    verbose: bool, default=False\n",
    "        Whether or not to log progress frequently.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    {'docno': docno, 'text': text}\n",
    "    \"\"\"\n",
    "    csv_file = open(filepath)\n",
    "    read_csv = csv.reader(csv_file, delimiter=delimiter)\n",
    "\n",
    "    for i, (docno, text) in enumerate(read_csv):\n",
    "        if i % 200000 == 0 and verbose:\n",
    "            print(f'Processing passage {i}')\n",
    "        yield {'docno': docno, 'text': text}\n",
    "\n",
    "\n",
    "\n",
    "def create_index():\n",
    "    \"\"\"\n",
    "    Creates pyterrier index using IterDictIndexer from pyterrier.\n",
    "    \"\"\"\n",
    "    if os.path.exists('index'):\n",
    "        shutil.rmtree('index')\n",
    "    index_path = os.path.join(os.getcwd(),'index')\n",
    "    iter_indexer = pt.IterDictIndexer(index_path)\n",
    "\n",
    "    collection_file = os.path.join(os.getcwd(),'data','collection.tsv')\n",
    "\n",
    "    doc_iter = passages_generator(collection_file, '\\t', verbose=True)\n",
    "    index_passages = iter_indexer.index(doc_iter)\n",
    "    print(\"done\")\n",
    "    return None\n",
    "    \n",
    "# uncomment to create index if index is not yet created.\n",
    "# create_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8myHdsIWyfg"
   },
   "source": [
    "# Connect to index, qrels, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMwG_070RdHh"
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "index_path = os.path.join(os.getcwd(),'index')\n",
    "qrels_path= os.path.join(os.getcwd(),'data')\n",
    "qrels_eval19_path = os.path.join(os.getcwd(),'data19', '2019qrels-pass.txt')\n",
    "qrels_train_path = os.path.join(qrels_path, 'qrels.train.tsv')\n",
    "qrels_dev_path = os.path.join(qrels_path, 'qrels.dev.tsv')\n",
    "\n",
    "topics_path = os.path.join(os.getcwd(),'data')\n",
    "topics_train_path = os.path.join(topics_path, 'queries.train.tsv')\n",
    "topics_dev_path = os.path.join(topics_path, 'queries.dev.tsv')\n",
    "topics_eval_path = os.path.join(topics_path, 'queries.eval.tsv')\n",
    "topics_eval19_path = os.path.join(os.getcwd(),'data19', 'queries.eval.tsv')\n",
    "\n",
    "# read data into dataframes from paths\n",
    "topics_train = pt.io.read_topics(topics_train_path, format='singleline')\n",
    "topics_dev = pt.io.read_topics(topics_dev_path, format='singleline')\n",
    "topics_eval = pt.io.read_topics(topics_eval_path, format='singleline')\n",
    "topics_eval19 = pt.io.read_topics(topics_eval19_path, format='singleline')\n",
    "\n",
    "qrels_train = pt.io.read_qrels(qrels_train_path)\n",
    "qrels_dev = pt.io.read_qrels(qrels_dev_path)\n",
    "qrels_eval19 = pt.io.read_qrels(qrels_eval19_path)\n",
    "\n",
    "indexRef = pt.TRECCollectionIndexer(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess empty topics\n",
    "\n",
    "some topics have empty queries for some reason, and that messes up pyterrier_bert so let's fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_empty_queries(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    fills all empty queries with some text so that pyterrier_bert does not crash.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy.loc[df_copy['query'].str.len() == 0,'query'] = 'nova'\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "topics_train = fill_empty_queries(topics_train)\n",
    "topics_dev = fill_empty_queries(topics_dev)\n",
    "topics_eval = fill_empty_queries(topics_eval)\n",
    "topics_eval19 = fill_empty_queries(topics_eval19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc2qOtjuW16n"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "VhAzxQEaPmV9",
    "outputId": "dc643baf-fd6a-4154-f46d-8d36c00c9adc"
   },
   "outputs": [],
   "source": [
    "def retrieve_from_multiple_models(indexref, models : list, num_results_per_model : int, query : str) -> pd.DataFrame:\n",
    "    first_run = True\n",
    "    all_dfs = None\n",
    "    for model in models:\n",
    "        if first_run:\n",
    "            all_dfs = pt.BatchRetrieve(indexref,controls={'wmodel':model}, num_results=num_results_per_model).transform(query)\n",
    "            all_dfs['model'] = model\n",
    "            first_run = False\n",
    "            continue\n",
    "        df = pt.BatchRetrieve(indexer,controls={'wmodel':model}, num_results=num_results_per_model).transform(query)\n",
    "        df['model'] = model\n",
    "        all_dfs = pd.concat([df, all_dfs])\n",
    "        break\n",
    "    all_dfs = all_dfs.set_index(['model','rank'])\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "all_retrieval_weighting_models = ['BB2', 'BM25', 'DFI0', 'DFR_BM25', 'DLH', \\\n",
    "                                  'DLH13', 'DPH', 'DFRee', 'Hiemstra_LM', \\\n",
    "                                  'DirichletLM', 'IFB2', 'In_expB2',\\\n",
    "                                  'In_expC2', 'InL2', 'LemurTF_IDF', 'LGD',\\\n",
    "                                  'PL2', 'TF_IDF', 'DFRWeightingModel']\n",
    "\n",
    "# run the following line to test retrieval\n",
    "# retrieve_from_multiple_models(indexRef, [all_retrieval_weighting_models[0]], 10, 'math')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITY8mx40A3FA"
   },
   "source": [
    "# Bert4IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "id": "fFgoJ-4W-n3s",
    "outputId": "bff626bc-f187-4461-bd7d-f406bab0248b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 13%|█▎        | 101638/808731 [2:19:17<15:54:59, 12.34q/s] "
     ]
    }
   ],
   "source": [
    "from pyterrier_bert.bert4ir import *\n",
    "\n",
    "results_path = os.path.join(os.getcwd(), 'results')\n",
    "\n",
    "BM25_br = pt.BatchRetrieve(indexRef, controls={\"wmodel\" : \"BM25\"}, verbose=True)\n",
    "bertpipe = BM25_br >> BERTPipeline()\n",
    "\n",
    "bertpipe.fit(topics_train, qrels_train, topics_dev, qrels_dev)\n",
    "\n",
    "\n",
    "\n",
    "# baseline for 2019 test set\n",
    "\"\"\"\n",
    "df_result_eval19 = pt.pipelines.Experiment([BM25_br],\n",
    "                        topics_dev,\n",
    "                        qrels_dev,\n",
    "                        ['map','ndcg'],  \n",
    "                        names=[\"BM25 + bert4ir\"])\n",
    "\n",
    "df_result_eval19.to_csv(\n",
    "    os.path.join(results_path, 'eval19_baseline_retrieval__bm25_bert4ir.csv'), \\\n",
    "    index=False)\n",
    "print('Finished eval 19')\n",
    "\"\"\"\n",
    "\n",
    "# baseline for 2020 eval\n",
    "\n",
    "df_baseline_retrieval_2020 = bertpipe.transform(topics_eval)\n",
    "df_baseline_retrieval_2020.to_csv(\n",
    "    os.path.join(results_path, 'eval20_baseline_retrieval__bm25_bert4ir.csv'), \\\n",
    "    index=False)\n",
    "print('Finished eval 20')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_bert.pyt_cedr import CEDRPipeline\n",
    "\n",
    "results_path = os.path.join(os.getcwd(), 'results')\n",
    "\n",
    "BM25_br = pt.BatchRetrieve(indexRef, controls={\"wmodel\" : \"BM25\"}, verbose=True)\n",
    "cedrpipe = BM25_br >> CEDRPipeline(max_valid_rank=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 732/808731 [01:17<23:38:56,  9.49q/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ed7f9cd07cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcedrpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25_br\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mCEDRPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_valid_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcedrpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m df_result_eval19 = pt.pipelines.Experiment([cedrpipe],\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyterrier/transformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, topics_or_res_tr, qrels_tr, topics_or_res_va, qrels_va)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_or_res_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics_or_res_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 \u001b[0mtopics_or_res_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_or_res_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m                 \u001b[0;31m# validation is optional for some learners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtopics_or_res_va\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyterrier/batchretrieve.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, queries)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# now ask Terrier to run the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunSearchRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyterrier_bert.pyt_cedr import CEDRPipeline\n",
    "from pyterrier_bert.bert4ir import *\n",
    "\n",
    "results_path = os.path.join(os.getcwd(), 'results')\n",
    "\n",
    "BM25_br = pt.BatchRetrieve(indexRef, controls={\"wmodel\" : \"BM25\"}, verbose=True)\n",
    "\n",
    "#cedr\n",
    "cedrpipe = BM25_br >> CEDRPipeline(max_valid_rank=20)\n",
    "\n",
    "cedrpipe.fit(topics_train, qrels_train, topics_dev, qrels_dev)\n",
    "\n",
    "df_baseline_retrieval_2020_cedr = cedrpipe.transform(topics_eval)\n",
    "df_baseline_retrieval_2020_cedr.to_csv(\n",
    "    os.path.join(results_path, 'eval20_baseline_retrieval__bm25_cedr.csv'), \\\n",
    "    index=False)\n",
    "print('done cedr 20')\n",
    "\n",
    "df_result_eval19_cedr = pt.pipelines.Experiment([cedrpipe],\n",
    "                        topics_dev,\n",
    "                        qrels_dev,\n",
    "                        ['map','ndcg'],  \n",
    "                        names=[\"BM25 + cedr\"])\n",
    "\n",
    "df_result_eval19.to_csv(\n",
    "    os.path.join(results_path, 'eval19_baseline_retrieval__bm25_cedr.csv'), \\\n",
    "    index=False)\n",
    "print('Finished cedr 19')\n",
    "\n",
    "# bert4ir\n",
    "\n",
    "bertpipe = BM25_br >> BERTPipeline(max_valid_rank=20)\n",
    "\n",
    "bertpipe.fit(topics_train, qrels_train, topics_dev, qrels_dev)\n",
    "\n",
    "df_baseline_retrieval_2020 = bertpipe.transform(topics_eval)\n",
    "df_baseline_retrieval_2020.to_csv(\n",
    "    os.path.join(results_path, 'eval20_baseline_retrieval__bm25_bert4ir.csv'), \\\n",
    "    index=False)\n",
    "print('done bert4ir 20')\n",
    "\n",
    "df_result_eval19_bert = pt.pipelines.Experiment([bertpipe],\n",
    "                        topics_dev,\n",
    "                        qrels_dev,\n",
    "                        ['map','ndcg'],  \n",
    "                        names=[\"BM25 + bert4ir\"])\n",
    "\n",
    "df_result_eval19_bert.to_csv(\n",
    "    os.path.join(results_path, 'eval19_baseline_retrieval__bm25_bert.csv'), \\\n",
    "    index=False)\n",
    "print('Finished bert4ir 19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_bert.pyt_cedr import CEDRPipeline\n",
    "\n",
    "cedrpipe = DPH_br >> CEDRPipeline(max_valid_rank=20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pyterrier - MWE trec DL 2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
